# CI/CD Pipeline Integration für Metropol Portal Load Tests
# Automatisierte Performance-Tests in verschiedenen Umgebungen
# Entwickelt von 2Brands Media GmbH

name: 🚀 Metropol Portal Load Tests

on:
  # Manuelle Ausführung
  workflow_dispatch:
    inputs:
      environment:
        description: 'Test Environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      test_suite:
        description: 'Test Suite'
        required: true
        default: 'all'
        type: choice  
        options:
          - all
          - critical
          - daily_scenarios
          - capacity
          - browser_simulation
      max_users:
        description: 'Maximum Concurrent Users'
        required: false
        default: '50'
        type: string
      
  # Geplante Ausführung
  schedule:
    # Täglich um 06:00 UTC (08:00 MEZ) - vor Arbeitsbeginn
    - cron: '0 6 * * *'
    # Wöchentlich Sonntags um 02:00 UTC - umfangreiche Tests
    - cron: '0 2 * * 0'

  # Bei Releases
  release:
    types: [published]

  # Bei Pull Requests in main branch
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

env:
  NODE_VERSION: '18'
  PHP_VERSION: '8.3'
  MYSQL_VERSION: '8.0'
  
  # Load Test Konfiguration
  BASE_URL_STAGING: 'https://staging.metropol-portal.com'
  BASE_URL_PRODUCTION: 'https://metropol-portal.com'
  
  # Performance Budgets (SLA-Ziele)
  SLA_LOGIN_TIME: 100          # Millisekunden
  SLA_ROUTE_CALC_TIME: 300     # Millisekunden
  SLA_STOP_UPDATE_TIME: 100    # Millisekunden
  SLA_ERROR_RATE: 5            # Prozent
  SLA_SUCCESS_RATE: 95         # Prozent

jobs:
  # Job 1: Setup und Validierung
  setup:
    name: 🔧 Setup und Validierung
    runs-on: ubuntu-latest
    outputs:
      base-url: ${{ steps.env-setup.outputs.base-url }}
      test-suite: ${{ steps.env-setup.outputs.test-suite }}
      max-users: ${{ steps.env-setup.outputs.max-users }}
      environment: ${{ steps.env-setup.outputs.environment }}
      
    steps:
      - name: 📁 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Setup Environment Variables
        id: env-setup
        run: |
          # Environment bestimmen
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            ENV="${{ github.event.inputs.environment }}"
            SUITE="${{ github.event.inputs.test_suite }}"
            USERS="${{ github.event.inputs.max_users }}"
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            if [[ "${{ github.event.schedule }}" == "0 6 * * *" ]]; then
              ENV="staging"
              SUITE="daily_scenarios"
              USERS="50"
            else
              ENV="staging" 
              SUITE="all"
              USERS="100"
            fi
          elif [[ "${{ github.event_name }}" == "release" ]]; then
            ENV="production"
            SUITE="critical"
            USERS="25"
          else
            ENV="staging"
            SUITE="critical"
            USERS="25"
          fi
          
          # Base URL setzen
          if [[ "$ENV" == "production" ]]; then
            BASE_URL="${{ env.BASE_URL_PRODUCTION }}"
          else
            BASE_URL="${{ env.BASE_URL_STAGING }}"
          fi
          
          echo "environment=$ENV" >> $GITHUB_OUTPUT
          echo "base-url=$BASE_URL" >> $GITHUB_OUTPUT
          echo "test-suite=$SUITE" >> $GITHUB_OUTPUT
          echo "max-users=$USERS" >> $GITHUB_OUTPUT
          
          echo "🎯 Target Environment: $ENV"
          echo "🌐 Base URL: $BASE_URL"
          echo "📋 Test Suite: $SUITE"
          echo "👥 Max Users: $USERS"
          
      - name: 🔍 Health Check
        run: |
          echo "🏥 Checking server health..."
          HEALTH_URL="${{ steps.env-setup.outputs.base-url }}/api/health"
          
          # Warten bis Server verfügbar ist (max 5 Minuten)
          for i in {1..30}; do
            if curl -f -s "$HEALTH_URL" > /dev/null; then
              echo "✅ Server is healthy"
              break  
            elif [[ $i -eq 30 ]]; then
              echo "❌ Server health check failed after 5 minutes"
              exit 1
            else
              echo "⏳ Waiting for server... (attempt $i/30)"
              sleep 10
            fi
          done

  # Job 2: Kritische Load Tests (immer ausführen)
  critical-tests:
    name: 🔴 Kritische Performance Tests
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJson('["all", "critical", "daily_scenarios"]'), needs.setup.outputs.test-suite)
    
    strategy:
      fail-fast: false
      matrix:
        scenario: [morningRush, lunchUpdate, eveningClose]
        
    steps:
      - name: 📁 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📦 Install Dependencies
        working-directory: tests/Performance
        run: |
          npm install -g k6
          npm install
          
      - name: 🚀 Run Critical Load Test - ${{ matrix.scenario }}
        working-directory: tests/Performance
        env:
          BASE_URL: ${{ needs.setup.outputs.base-url }}
          SCENARIO: ${{ matrix.scenario }}
          MAX_USERS: ${{ needs.setup.outputs.max-users }}
        run: |
          echo "🎯 Running ${{ matrix.scenario }} scenario"
          echo "🌐 Target: $BASE_URL"
          echo "👥 Max Users: $MAX_USERS"
          
          # K6 mit JSON-Output für weitere Verarbeitung
          k6 run \
            --out json=results-${{ matrix.scenario }}.json \
            --summary-trend-stats="min,avg,med,max,p(95),p(99)" \
            --env BASE_URL="$BASE_URL" \
            --env SCENARIO="$SCENARIO" \
            --env MAX_USERS="$MAX_USERS" \
            k6-realtime-scenarios.js
            
      - name: 📊 Parse Results
        working-directory: tests/Performance
        run: |
          echo "📊 Parsing results for ${{ matrix.scenario }}"
          
          # Extrahiere wichtige Metriken aus K6 JSON Output
          node -e "
            const fs = require('fs');
            const results = fs.readFileSync('results-${{ matrix.scenario }}.json', 'utf8')
              .split('\n')
              .filter(line => line.trim())
              .map(line => { try { return JSON.parse(line); } catch { return null; } })
              .filter(data => data && data.type === 'Point');
            
            const metrics = {};
            results.forEach(point => {
              const metric = point.metric;
              const value = point.data.value;
              if (!metrics[metric]) metrics[metric] = [];
              metrics[metric].push(value);
            });
            
            // Berechne Statistiken
            Object.entries(metrics).forEach(([name, values]) => {
              if (values.length === 0) return;
              const sorted = values.sort((a, b) => a - b);
              const len = sorted.length;
              const stats = {
                min: sorted[0],
                max: sorted[len - 1],
                avg: values.reduce((a, b) => a + b, 0) / len,
                p95: sorted[Math.floor(len * 0.95)],
                p99: sorted[Math.floor(len * 0.99)]
              };
              console.log(\`📈 \${name}: avg=\${stats.avg.toFixed(2)}ms, p95=\${stats.p95.toFixed(2)}ms, p99=\${stats.p99.toFixed(2)}ms\`);
              
              // SLA-Validierung
              const slaTargets = {
                'login_time': ${{ env.SLA_LOGIN_TIME }},
                'route_calculation_time': ${{ env.SLA_ROUTE_CALC_TIME }},
                'stop_update_time': ${{ env.SLA_STOP_UPDATE_TIME }}
              };
              
              if (slaTargets[name] && stats.p95 > slaTargets[name]) {
                console.error(\`❌ SLA-Verletzung \${name}: P95 \${stats.p95.toFixed(2)}ms > Ziel \${slaTargets[name]}ms\`);
                process.exit(1);
              } else if (slaTargets[name]) {
                console.log(\`✅ SLA erfüllt \${name}: P95 \${stats.p95.toFixed(2)}ms <= Ziel \${slaTargets[name]}ms\`);
              }
            });
          "
          
      - name: 📁 Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: critical-test-results-${{ matrix.scenario }}
          path: tests/Performance/results-${{ matrix.scenario }}.json
          retention-days: 30

  # Job 3: Kapazitäts-Tests (nur bei vollständiger Test-Suite)
  capacity-tests:
    name: 💪 Kapazitäts-Tests
    runs-on: ubuntu-latest
    needs: [setup, critical-tests]
    if: contains(fromJson('["all", "capacity"]'), needs.setup.outputs.test-suite)
    
    strategy:
      fail-fast: false
      matrix:
        scenario: [normalLoad, peakLoad, stressTest]
        
    steps:
      - name: 📁 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📦 Install Dependencies  
        working-directory: tests/Performance
        run: |
          npm install -g k6
          npm install
          
      - name: 💪 Run Capacity Test - ${{ matrix.scenario }}
        working-directory: tests/Performance
        env:
          BASE_URL: ${{ needs.setup.outputs.base-url }}
          SCENARIO: ${{ matrix.scenario }}
          MAX_USERS: ${{ needs.setup.outputs.max-users }}
        run: |
          echo "💪 Running ${{ matrix.scenario }} capacity test"
          
          # Angepasste User-Limits für verschiedene Szenarien
          case "${{ matrix.scenario }}" in
            "normalLoad")
              USERS=25
              ;;
            "peakLoad")
              USERS=100
              ;;
            "stressTest")
              USERS=200
              ;;
          esac
          
          k6 run \
            --out json=capacity-results-${{ matrix.scenario }}.json \
            --summary-trend-stats="min,avg,med,max,p(95),p(99)" \
            --env BASE_URL="$BASE_URL" \
            --env SCENARIO="$SCENARIO" \
            --env MAX_USERS="$USERS" \
            k6-realtime-scenarios.js
            
      - name: 📁 Upload Capacity Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: capacity-test-results-${{ matrix.scenario }}
          path: tests/Performance/capacity-results-${{ matrix.scenario }}.json
          retention-days: 30

  # Job 4: Browser-Simulation (nur bei vollständiger Test-Suite)
  browser-simulation:
    name: 🌐 Browser-Simulation Tests
    runs-on: ubuntu-latest
    needs: [setup, critical-tests]
    if: contains(fromJson('["all", "browser_simulation"]'), needs.setup.outputs.test-suite)
    
    steps:
      - name: 📁 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📦 Install Dependencies
        working-directory: tests/Performance
        run: |
          npm install
          npx playwright install chromium firefox webkit
          
      - name: 🌐 Run Browser Load Tests
        working-directory: tests/Performance
        env:
          BASE_URL: ${{ needs.setup.outputs.base-url }}
          CONCURRENT_USERS: 25
          DURATION: 180
        run: |
          echo "🌐 Running browser simulation tests"
          
          # Playwright Tests mit Load-Simulation
          npx playwright test playwright-load-scenarios.ts \
            --reporter=json \
            --output=browser-test-results.json
            
      - name: 📁 Upload Browser Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: browser-simulation-results
          path: tests/Performance/browser-test-results.json
          retention-days: 30

  # Job 5: Comprehensive Report Generation
  generate-report:
    name: 📊 Generate Comprehensive Report
    runs-on: ubuntu-latest
    needs: [setup, critical-tests]
    if: always() && (needs.critical-tests.result == 'success' || needs.critical-tests.result == 'failure')
    
    steps:
      - name: 📁 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: 📦 Install Dependencies
        working-directory: tests/Performance
        run: npm install
        
      - name: 📥 Download All Test Results
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/
          
      - name: 📊 Generate Comprehensive Report
        working-directory: tests/Performance
        env:
          BASE_URL: ${{ needs.setup.outputs.base-url }}
          ENVIRONMENT: ${{ needs.setup.outputs.environment }}
          TEST_SUITE: ${{ needs.setup.outputs.test-suite }}
        run: |
          echo "📊 Generating comprehensive performance report"
          
          # Alle Ergebnisse zusammenführen
          mkdir -p consolidated-results
          find ../test-artifacts -name "*.json" -exec cp {} consolidated-results/ \;
          
          # Report-Generator ausführen
          node -e "
            const LoadTestRunner = require('./load-test-runner.ts').default;
            const runner = new LoadTestRunner('$BASE_URL', './consolidated-results');
            
            // Simuliere Report-Generierung basierend auf gesammelten Daten
            const reportData = {
              summary: {
                environment: '$ENVIRONMENT',
                testSuite: '$TEST_SUITE',
                timestamp: new Date().toISOString(),
                baseUrl: '$BASE_URL'
              },
              slaCompliance: {
                loginTime: { target: ${{ env.SLA_LOGIN_TIME }}, status: 'unknown' },
                routeCalculation: { target: ${{ env.SLA_ROUTE_CALC_TIME }}, status: 'unknown' },
                stopUpdate: { target: ${{ env.SLA_STOP_UPDATE_TIME }}, status: 'unknown' }
              },
              recommendations: [
                'Überprüfen Sie die Ergebnisse der einzelnen Test-Szenarien',
                'Implementieren Sie empfohlene Performance-Optimierungen',
                'Überwachen Sie System-Metriken während Peak-Zeiten'
              ]
            };
            
            console.log('📈 Performance Report Summary:');
            console.log('Environment:', reportData.summary.environment);
            console.log('Test Suite:', reportData.summary.testSuite);
            console.log('Target URL:', reportData.summary.baseUrl);
            console.log('SLA Targets:');
            console.log('  - Login Time: <= ${SLA_LOGIN_TIME}ms');
            console.log('  - Route Calculation: <= ${SLA_ROUTE_CALC_TIME}ms');
            console.log('  - Stop Update: <= ${SLA_STOP_UPDATE_TIME}ms');
          "
          
      - name: 📁 Upload Comprehensive Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-performance-report
          path: tests/Performance/consolidated-results/
          retention-days: 90

  # Job 6: Performance Monitoring & Alerts
  performance-monitoring:
    name: 📈 Performance Monitoring
    runs-on: ubuntu-latest
    needs: [setup, critical-tests, generate-report]
    if: always()
    
    steps:
      - name: 📊 Performance Metrics Analysis
        run: |
          echo "📈 Analyzing performance trends..."
          
          # Hier könnten externe Monitoring-Tools integriert werden
          # z.B. New Relic, DataDog, Custom Prometheus Metrics
          
          echo "🎯 SLA Targets:"
          echo "  - Login Time: <= ${{ env.SLA_LOGIN_TIME }}ms"
          echo "  - Route Calculation: <= ${{ env.SLA_ROUTE_CALC_TIME }}ms"
          echo "  - Stop Update: <= ${{ env.SLA_STOP_UPDATE_TIME }}ms"
          echo "  - Error Rate: <= ${{ env.SLA_ERROR_RATE }}%"
          echo "  - Success Rate: >= ${{ env.SLA_SUCCESS_RATE }}%"
          
      - name: 🚨 Send Alerts (if needed)
        if: needs.critical-tests.result == 'failure'
        run: |
          echo "🚨 Critical performance tests failed!"
          echo "Environment: ${{ needs.setup.outputs.environment }}"
          echo "Base URL: ${{ needs.setup.outputs.base-url }}"
          echo "Test Suite: ${{ needs.setup.outputs.test-suite }}"
          
          # Hier könnten Slack/Teams/Email-Benachrichtigungen gesendet werden
          # curl -X POST -H 'Content-type: application/json' \
          #   --data '{"text":"🚨 Metropol Portal Performance Alert: Critical tests failed in ${{ needs.setup.outputs.environment }}"}' \
          #   $SLACK_WEBHOOK_URL

  # Job 7: Cleanup
  cleanup:
    name: 🧹 Cleanup
    runs-on: ubuntu-latest
    needs: [performance-monitoring]
    if: always()
    
    steps:
      - name: 🧹 Cleanup temporary resources
        run: |
          echo "🧹 Cleaning up temporary test resources..."
          
          # Hier könnten temporäre Test-Datenbanken, Caches etc. bereinigt werden
          echo "✅ Cleanup completed"
          
      - name: 📝 Summary Report
        run: |
          echo "📋 Load Test Pipeline Summary"
          echo "=============================="
          echo "Environment: ${{ needs.setup.outputs.environment }}"
          echo "Base URL: ${{ needs.setup.outputs.base-url }}"
          echo "Test Suite: ${{ needs.setup.outputs.test-suite }}"
          echo "Max Users: ${{ needs.setup.outputs.max-users }}"
          echo ""
          echo "Job Results:"
          echo "  - Setup: ${{ needs.setup.result }}"
          echo "  - Critical Tests: ${{ needs.critical-tests.result }}"
          echo "  - Generate Report: ${{ needs.generate-report.result }}"
          echo "  - Performance Monitoring: ${{ needs.performance-monitoring.result }}"
          echo ""
          echo "Entwickelt von 2Brands Media GmbH"
          echo "LoadTestAgent v1.0.0"